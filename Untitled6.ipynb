{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "4WJx8kYrW5Dt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('meta_speaker_stats.csv')\n",
        "\n",
        "# Filter rows where the 'language' column is 'Gujarati'\n",
        "df_gujarati = df[df['primary_language'] == 'Gujarati']\n",
        "\n",
        "# Save the filtered data to a new CSV file if needed\n",
        "df_gujarati.to_csv('filtered_gujarati.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ovJyBpz_XxxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRbpBWWIY3Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Load the CSV containing audio file paths and languages\n",
        "metadata_df = pd.read_csv('filtered_gujarati.csv')\n",
        "\n",
        "# Filter for Gujarati language\n",
        "gujarati_audio_files = metadata_df[metadata_df['primary_language'] == 'Gujarati']['audio_filepath']\n",
        "\n",
        "# Directory where all audio files are stored in Google Drive\n",
        "source_directory = '/content/drive/MyDrive'\n",
        "\n",
        "# Directory where you want to save only Gujarati audio files\n",
        "destination_directory = '/content/drive/MyDrive/OnlyGujarati'\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "# Copy each Gujarati audio file to the new directory\n",
        "for file_name in gujarati_audio_files:\n",
        "    source_path = os.path.join(source_directory, file_name)\n",
        "    destination_path = os.path.join(destination_directory, file_name)\n",
        "\n",
        "    # Create necessary subdirectories in the destination path\n",
        "    os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
        "\n",
        "    # Check if file exists before copying\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.copy(source_path, destination_path)\n",
        "    else:\n",
        "        print(f\"File not found: {source_path}\")\n"
      ],
      "metadata": {
        "id": "iwfEsyfsXcy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/drive/MyDrive/OnlyGujarati'  # Directory containing audio files\n",
        "metadata_file = '/content/filtered_gujarati.csv'  # CSV containing text and audio file names\n",
        "output_dir = 'output_data'  # Directory to save processed audio files\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv(metadata_file)\n",
        "\n",
        "# Display the first few rows of the metadata\n",
        "print(metadata.head())\n",
        "\n",
        "# Text normalization function\n",
        "def normalize_text(text):\n",
        "    # Implement normalization rules (e.g., convert numbers to words, remove special characters)\n",
        "    # Example: Replace digits with their Gujarati words, and remove unwanted characters\n",
        "    normalized_text = text.replace(\"1\", \"એક\").replace(\"2\", \"બે\")  # Add more replacements as needed\n",
        "    normalized_text = ''.join(c for c in normalized_text if c.isalnum() or c.isspace())\n",
        "    return normalized_text.strip()\n",
        "\n",
        "# Normalize text and process audio files\n",
        "normalized_data = []\n",
        "\n",
        "for index, row in metadata.iterrows():\n",
        "    audio_file = os.path.join(data_dir, row['audio_filepath'])  # Update based on your metadata structure\n",
        "    text = normalize_text(row['text'])\n",
        "\n",
        "    # Load audio file\n",
        "    try:\n",
        "        audio, sr = librosa.load(audio_file, sr=16000)  # Resample to 16kHz\n",
        "        # Normalize audio: Optionally, you can apply more preprocessing steps like trimming silence or volume normalization\n",
        "        audio = librosa.util.normalize(audio)\n",
        "\n",
        "        # Save the processed audio file\n",
        "        output_file = os.path.join(output_dir, f\"processed_{index}.wav\")\n",
        "        sf.write(output_file, audio, sr)\n",
        "\n",
        "        # Append to the normalized data list\n",
        "        normalized_data.append({'text': text, 'audio_file': output_file})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {e}\")\n",
        "\n",
        "# Create a DataFrame for the normalized data\n",
        "normalized_df = pd.DataFrame(normalized_data)\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(normalized_df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save the split datasets\n",
        "train_df.to_csv(os.path.join(output_dir, 'train_data.csv'), index=False)\n",
        "val_df.to_csv(os.path.join(output_dir, 'val_data.csv'), index=False)\n",
        "test_df.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False)\n",
        "\n",
        "print(\"Data preparation completed!\")\n",
        "print(f\"Training data saved to {output_dir}/train_data.csv\")\n",
        "print(f\"Validation data saved to {output_dir}/val_data.csv\")\n",
        "print(f\"Test data saved to {output_dir}/test_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uib10gESmQi",
        "outputId": "ad01554c-b09d-4021-97e5-7c0a6a86a3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            audio_filepath  duration   text gender age-group  \\\n",
            "0  audio/281474976896933_f3077_chunk_0.wav  0.293500  Agree   Male     18-30   \n",
            "1  audio/281474976896932_f2663_chunk_0.wav  0.296375    Yes   Male     18-30   \n",
            "2  audio/281474976896928_f2744_chunk_0.wav  0.361125     On   Male     18-30   \n",
            "3  audio/281474976896935_f1040_chunk_0.wav  0.366875   Help   Male     18-30   \n",
            "4  audio/281474976896930_f1041_chunk_0.wav  0.413250   Nine   Male     18-30   \n",
            "\n",
            "  primary_language native_place_state native_place_district  \\\n",
            "0         Gujarati            Gujarat           Sabarkantha   \n",
            "1         Gujarati            Gujarat           Sabarkantha   \n",
            "2         Gujarati            Gujarat           Sabarkantha   \n",
            "3         Gujarati            Gujarat           Sabarkantha   \n",
            "4         Gujarati            Gujarat           Sabarkantha   \n",
            "\n",
            "  highest_qualification job_category      occupation_domain  \n",
            "0         Post Graduate    Full Time  Information and Media  \n",
            "1         Post Graduate    Full Time  Information and Media  \n",
            "2         Post Graduate    Full Time  Information and Media  \n",
            "3         Post Graduate    Full Time  Information and Media  \n",
            "4         Post Graduate    Full Time  Information and Media  \n",
            "Data preparation completed!\n",
            "Training data saved to output_data/train_data.csv\n",
            "Validation data saved to output_data/val_data.csv\n",
            "Test data saved to output_data/test_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torchaudio\n",
        "!git clone https://github.com/jaywalnut310/vits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NApYp0K0UxN_",
        "outputId": "d95ae1bc-7b2b-4f9c-cfc6-6d545a245995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Cloning into 'vits'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 81 (from 1)\u001b[K\n",
            "Receiving objects: 100% (81/81), 3.33 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uTxKYAQwXOHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/vits')  # Update this to the path where VITS is located"
      ],
      "metadata": {
        "id": "rS7kPkH5Vj46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/vits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEIFkLVXXPPY",
        "outputId": "616a41a5-831d-4ac4-c75f-effa224db9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attentions.py  filelists\tmel_processing.py  preprocess.py     resources\t  transforms.py\n",
            "commons.py     inference.ipynb\tmodels.py\t   __pycache__\t     text\t  utils.py\n",
            "configs        LICENSE\t\tmodules.py\t   README.md\t     train_ms.py\n",
            "data_utils.py  losses.py\tmonotonic_align    requirements.txt  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio librosa soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxzpAUMJVueC",
        "outputId": "4d8679b3-09ad-4f3c-8bec-fbd6c0298c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monotonic-align"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQx2rjwCZvPk",
        "outputId": "68464bf1-c7ba-4c24-b037-8c843cc51446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monotonic-align\n",
            "  Downloading monotonic_align-1.0.0.tar.gz (4.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: monotonic-align\n",
            "  Building wheel for monotonic-align (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for monotonic-align: filename=monotonic_align-1.0.0-cp310-cp310-linux_x86_64.whl size=305266 sha256=efc11003f73acd13fbe7fe86206a44c1f86f1a9923f27d058e60458107904908\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/6b/3f/b627321313060822a9a1cde0b07e5d78728d80a290b6a5496a\n",
            "Successfully built monotonic-align\n",
            "Installing collected packages: monotonic-align\n",
            "Successfully installed monotonic-align-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode phonemizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x01n0j36v20K",
        "outputId": "f5634a99-95c6-4c47-f2d1-d05d3c4c0643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting phonemizer\n",
            "  Downloading phonemizer-3.3.0-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from phonemizer) (1.4.2)\n",
            "Collecting segments (from phonemizer)\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer) (24.2.0)\n",
            "Collecting dlinfo (from phonemizer)\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from phonemizer) (4.12.2)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer)\n",
            "  Downloading clldutils-3.24.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting csvw>=1.5.6 (from segments->phonemizer)\n",
            "  Downloading csvw-3.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (2024.9.11)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Downloading bibtexparser-2.0.0b7-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (3.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (5.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (3.0.2)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.32.3)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting rdflib (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->clldutils>=1.7.3->segments->phonemizer) (1.16.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2024.8.30)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonemizer-3.3.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Downloading clldutils-3.24.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading csvw-3.5.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bibtexparser-2.0.0b7-py3-none-any.whl (38 kB)\n",
            "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.1-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=31d166f5bf199c8f28e796b200ef494b36792e58a9fc3ef1a76759f795f83bc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: rfc3986, pylatexenc, language-tags, dlinfo, unidecode, isodate, colorlog, colorama, bibtexparser, rdflib, clldutils, csvw, segments, phonemizer\n",
            "Successfully installed bibtexparser-2.0.0b7 clldutils-3.24.0 colorama-0.4.6 colorlog-6.9.0 csvw-3.5.1 dlinfo-1.2.1 isodate-0.7.2 language-tags-1.2.0 phonemizer-3.3.0 pylatexenc-2.10 rdflib-7.1.1 rfc3986-1.5.0 segments-2.2.1 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch librosa pandas soundfile matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVHgxX0yv73s",
        "outputId": "6b6892f2-48fd-4997-8490-cff1518930b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from vits import commons, utils\n",
        "from vits.models import SynthesizerTrn\n",
        "from vits.train import train as train_model\n",
        "from vits.utils import load_data\n",
        "\n",
        "# Configure paths\n",
        "output_dir = \"output_data\"\n",
        "train_data_path = os.path.join(output_dir, \"train_data.csv\")\n",
        "val_data_path = os.path.join(output_dir, \"val_data.csv\")\n",
        "test_data_path = os.path.join(output_dir, \"test_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "NeZumtlKwGAJ",
        "outputId": "64926010-d064-4845-cac6-3ffed098f29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'train' from 'vits.train' (/content/vits/train.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-730b732a5af7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvits\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSynthesizerTrn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'train' from 'vits.train' (/content/vits/train.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/vits/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4F-m5YbxH_T",
        "outputId": "8e71d112-6c91-4b94-b295-2a579b4e5e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Cython==0.29.21 (from -r /content/vits/requirements.txt (line 1))\n",
            "  Downloading Cython-0.29.21-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting librosa==0.8.0 (from -r /content/vits/requirements.txt (line 2))\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/183.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==3.3.1 (from -r /content/vits/requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.3.1.tar.gz (38.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.18.5 (from -r /content/vits/requirements.txt (line 4))\n",
            "  Downloading numpy-1.18.5.zip (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from vits import commons, utils\n",
        "from vits.models import SynthesizerTrn\n",
        "from vits.train import train_and_evaluate as train_model\n",
        "from vits.utils import load_wav_to_torch\n",
        "\n",
        "# Configure paths\n",
        "output_dir = \"output_data\"\n",
        "train_data_path = os.path.join(output_dir, \"train_data.csv\")\n",
        "val_data_path = os.path.join(output_dir, \"val_data.csv\")\n",
        "test_data_path = os.path.join(output_dir, \"test_data.csv\")"
      ],
      "metadata": {
        "id": "Vor5BV_cxSkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the CSV files directly\n",
        "train_data = pd.read_csv(\"/content/output_data/train_data.csv\")  # Replace with actual path\n",
        "val_data = pd.read_csv(\"/content/output_data/val_data.csv\")  # Replace with actual path\n",
        "\n",
        "# Initialize lists to store the data\n",
        "train_texts = []\n",
        "train_audios = []\n",
        "val_texts = []\n",
        "val_audios = []\n",
        "\n",
        "# Set a fixed length for padding or use the longest sequence\n",
        "fixed_audio_length = 16000 * 10  # 10 seconds of audio at 16kHz (for example)\n",
        "\n",
        "# Process the training data\n",
        "for idx in range(len(train_data)):\n",
        "    text = train_data.iloc[idx][\"text\"]\n",
        "    audio_path = train_data.iloc[idx][\"audio_file\"]\n",
        "\n",
        "    # Load the audio\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Pad the audio sequence to the fixed length\n",
        "    if len(audio) < fixed_audio_length:\n",
        "        padding = fixed_audio_length - len(audio)\n",
        "        audio = F.pad(torch.FloatTensor(audio), (0, padding))\n",
        "    else:\n",
        "        audio = audio[:fixed_audio_length]  # Truncate to fixed length if too long\n",
        "\n",
        "    # Optionally: Transform text into a tensor (e.g., tokenization, padding)\n",
        "    # Here, we just keep it as raw text (you can modify this part as needed)\n",
        "    train_texts.append(text)\n",
        "    train_audios.append(audio)\n",
        "\n",
        "# Process the validation data similarly\n",
        "for idx in range(len(val_data)):\n",
        "    text = val_data.iloc[idx][\"text\"]\n",
        "    audio_path = val_data.iloc[idx][\"audio_file\"]\n",
        "\n",
        "    # Load the audio\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Pad the audio sequence to the fixed length\n",
        "    if len(audio) < fixed_audio_length:\n",
        "        padding = fixed_audio_length - len(audio)\n",
        "        audio = F.pad(torch.FloatTensor(audio), (0, padding))\n",
        "    else:\n",
        "        audio = audio[:fixed_audio_length]  # Truncate to fixed length if too long\n",
        "\n",
        "    val_texts.append(text)\n",
        "    val_audios.append(audio)\n",
        "\n",
        "# Convert audio lists into tensors\n",
        "train_audio_tensor = torch.stack([torch.FloatTensor(audio) for audio in train_audios])\n",
        "val_audio_tensor = torch.stack([torch.FloatTensor(audio) for audio in val_audios])\n",
        "\n",
        "# Convert texts into tensor format (you will need to tokenize or encode text based on your model)\n",
        "train_text_tensor = torch.tensor([len(text) for text in train_texts])  # Example: Length of text as a placeholder\n",
        "val_text_tensor = torch.tensor([len(text) for text in val_texts])  # Example: Length of text as a placeholder\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_text_tensor, train_audio_tensor)\n",
        "val_dataset = TensorDataset(val_text_tensor, val_audio_tensor)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Example of how to access the data\n",
        "for text, audio in train_loader:\n",
        "    print(f\"Text: {text}, Audio shape: {audio.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zKiqn_X0DYR",
        "outputId": "e8779d51-28cc-4423-e58f-cdcb169ec920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: tensor([ 91,  69,  33,  18,  19,  18,  28,  50,  13, 100, 113,  10,   7,  85,\n",
            "         66,  46]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([ 37, 103,  28, 301, 131,  22,  69, 111, 134,  40,  70,  31,  34,  50,\n",
            "         50,  21]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([  5,  22,  36, 151,  61, 126, 109,  29,  60,   3,  66,  23,  65,   5,\n",
            "         56,  52]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([102,  67,  42,  86,  85,  24,  73,  73, 196,  42, 139,  38,  41,  91,\n",
            "         18,  33]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([ 69,  56,  51, 137,  46,   4,  90,  91,  69,  34,  34,  73,  54,  24,\n",
            "         57,  18]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([ 10,   3,  76,  44,  83,  62,  88,   4,  67,  22,  23, 159,  33,  69,\n",
            "          5,  92]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([ 58,  69,  62,  19,   3,  52,  73,  15,  34,  42,  78,  61, 125,  52,\n",
            "          4,   5]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([106,  65,  40, 174,  44,  46,  91,  11, 128,  62,  86, 184,  91,  49,\n",
            "         50,  33]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([  8,  80,  46,   6,  75,  54,  87, 116,   3,  85,  93,  46, 118,  28,\n",
            "         69, 103]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([103, 121,  63,  71,  67,  60,  27,  30,  53, 141,  59,  71,  67,  48,\n",
            "          3,  63]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([ 18,  47,  45, 161,  13,  62,  47, 110,  92,  75,  29,  86,   7,  99,\n",
            "         86,  10]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([ 78,  69,   4, 110, 115,  13,   2,  45,  77,  32,  76,  35,  55,  45,\n",
            "         44, 107]), Audio shape: torch.Size([16, 160000])\n",
            "Text: tensor([56]), Audio shape: torch.Size([1, 160000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initial Gujarati Vocab List (Don't worry about missing characters now)\n",
        "gujarati_vocab = [\n",
        "    # Gujarati Vowels\n",
        "    \"અ\", \"આ\", \"ઇ\", \"ઈ\", \"ઉ\", \"ઊ\", \"ઋ\", \"ૠ\", \"ઑ\", \"ઔ\", \"ે\",\n",
        "\n",
        "    # Gujarati Consonants\n",
        "    \"ક\", \"ખ\", \"ગ\", \"ઘ\", \"ઙ\", \"ચ\", \"છ\", \"જ\", \"ઝ\", \"ઞ\", \"ટ\", \"ઠ\", \"ડ\", \"ઢ\",\n",
        "    \"ણ\", \"ત\", \"થ\", \"દ\", \"ધ\", \"ન\", \"પ\", \"ફ\", \"બ\", \"ભ\", \"મ\", \"ય\", \"ર\", \"લ\", \"વ\",\n",
        "    \"શ\", \"ષ\", \"સ\", \"હ\", \"ળ\", \"ક્ષ\", \"ઙ્\", \"્\",  # Added '્' (halant/virama)\n",
        "\n",
        "    # Gujarati Numerals\n",
        "    \"૦\", \"૧\", \"૨\", \"૩\", \"૪\", \"૫\", \"૬\", \"૭\", \"૮\", \"૯\"\n",
        "]  # Ensure all characters are listed here\n",
        "\n",
        "# Create a label encoder to encode text into integer tokens\n",
        "vocab_encoder = LabelEncoder()\n",
        "vocab_encoder.fit(gujarati_vocab)\n",
        "\n",
        "def encode_text(text):\n",
        "    encoded_text = []\n",
        "\n",
        "    # Loop through each character in the input text\n",
        "    for char in text:\n",
        "        if char not in vocab_encoder.classes_:\n",
        "            # If character is not in the vocabulary, add it dynamically\n",
        "            vocab_encoder.classes_ = list(vocab_encoder.classes_) + [char]\n",
        "            vocab_encoder.fit(vocab_encoder.classes_)\n",
        "\n",
        "        # Encode the character\n",
        "        encoded_text.append(vocab_encoder.transform([char])[0])\n",
        "\n",
        "    return encoded_text\n",
        "\n",
        "# Example: Encoding a sentence (Gujarati text)\n",
        "text = \"હેલ્લો\"\n",
        "encoded_text = encode_text(text)  # Dynamically encode each character\n",
        "\n",
        "# Print the encoded result\n",
        "print(\"Encoded text:\", encoded_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEpwUZSv3Fg7",
        "outputId": "ab551ce1-ade1-4651-c9ee-4d4ea99f9267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded text: [44, 45, 38, 46, 38, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vits.models import SynthesizerTrn  # Assuming your model class is defined here\n",
        "\n",
        "# Initialize model with parameters suited to the Gujarati language\n",
        "# Initialize model with required parameters\n",
        "model = SynthesizerTrn(\n",
        "    n_vocab=len(vocab_encoder.classes_),  # Set number of unique tokens in your vocabulary\n",
        "    spec_channels=80,\n",
        "    segment_size=8192,\n",
        "    inter_channels=192,\n",
        "    hidden_channels=192,\n",
        "    filter_channels=768,\n",
        "    resblock_kernel_sizes=[3, 7, 11],\n",
        "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
        "    upsample_rates=[8, 8, 2, 2],\n",
        "    upsample_initial_channel=512,\n",
        "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
        "\n",
        "    # New required parameters for initialization\n",
        "    n_heads=8,                  # Example value\n",
        "    n_layers=6,                 # Example value\n",
        "    kernel_size=3,              # Example value\n",
        "    p_dropout=0.1,              # Example value\n",
        "    resblock=64                 # Example value\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.cuda() if torch.cuda.is_available() else model"
      ],
      "metadata": {
        "id": "u9yY7sAj7I3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# MSELoss for training\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Optionally, if using VITS-like models, you might want a more complex loss function\n",
        "# For example, the VITS model uses multiple losses like L1 loss, reconstruction loss, adversarial loss, etc."
      ],
      "metadata": {
        "id": "pSwSg-DM8ekg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text indices range:\", text.min().item(), text.max().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHTYfqE9iln",
        "outputId": "e5766217-bbf4-4bec-cb8f-7f689d94c3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text indices range: 4 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "for i, (text, audio) in enumerate(train_loader):\n",
        "    # ... (other code)\n",
        "\n",
        "    # Check if text indices are within vocabulary bounds\n",
        "    max_index = text.max().item()\n",
        "    if max_index >= model.enc_p.emb.num_embeddings:  # Adjust to access the embedding layer\n",
        "        print(f\"Warning: Text tensor contains index {max_index} outside vocabulary range (0-{model.enc_p.emb.num_embeddings - 1})\")\n",
        "\n",
        "        # Option 1: Clip the indices to be within the vocabulary range\n",
        "        text = torch.clamp(text, 0, model.enc_p.emb.num_embeddings - 1)\n",
        "\n",
        "        # Option 2: Skip the current batch (use with caution)\n",
        "        # continue\n",
        "\n",
        "    # ... (rest of your training loop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOW9f4-BANWC",
        "outputId": "c843c81c-e008-4449-c930-a477a611a442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Text tensor contains index 141 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 184 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 131 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 301 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 110 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 128 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 196 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 100 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 116 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 139 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 109 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 174 outside vocabulary range (0-58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "for i, (text, audio) in enumerate(train_loader):\n",
        "    # ... (other code)\n",
        "\n",
        "    # Check if text indices are within vocabulary bounds\n",
        "    max_index = text.max().item()\n",
        "    vocab_size = model.enc_p.emb.num_embeddings  # Get the vocabulary size\n",
        "    if max_index >= vocab_size:\n",
        "        print(f\"Warning: Text tensor contains index {max_index} outside vocabulary range (0-{vocab_size - 1})\")\n",
        "\n",
        "        # Clip the indices to be within the vocabulary range\n",
        "        text = torch.clamp(text, 0, vocab_size - 1)\n",
        "\n",
        "    # ... (rest of your training loop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1LaHE60AdcI",
        "outputId": "be569f01-9aeb-423a-a9cd-a3b1c28a463f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Text tensor contains index 301 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 184 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 125 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 196 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 103 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 174 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 115 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 161 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 103 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 134 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 139 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 77 outside vocabulary range (0-58)\n",
            "Warning: Text tensor contains index 88 outside vocabulary range (0-58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Example of a simple model (just for illustration)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 2)  # Example layer (10 input features, 2 output classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleModel().cuda() if torch.cuda.is_available() else SimpleModel()\n",
        "\n",
        "# Example optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Dummy data (just for illustration, you can replace this with your real data loader)\n",
        "# Assuming input is a tensor of shape (batch_size, 10) and labels are a tensor of shape (batch_size)\n",
        "train_data = torch.randn(100, 10).cuda() if torch.cuda.is_available() else torch.randn(100, 10)  # 100 samples, 10 features\n",
        "train_labels = torch.randint(0, 2, (100,)).cuda() if torch.cuda.is_available() else torch.randint(0, 2, (100,))\n",
        "\n",
        "# Basic training loop\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_data), 32):  # Using a batch size of 32\n",
        "        # Get batch data\n",
        "        inputs = train_data[i:i+32]\n",
        "        labels = train_labels[i:i+32]\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute predictions\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print loss for the epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_data):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPjP6yytArCI",
        "outputId": "379baff6-6ba3-4ede-b2ed-e5025a2ce14b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0301\n",
            "Epoch [2/5], Loss: 0.0300\n",
            "Epoch [3/5], Loss: 0.0299\n",
            "Epoch [4/5], Loss: 0.0297\n",
            "Epoch [5/5], Loss: 0.0296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Turn off gradients for evaluation (saves memory)\n",
        "    # Assuming you have a test set:\n",
        "    test_data = torch.randn(100, 10).cuda() if torch.cuda.is_available() else torch.randn(100, 10)\n",
        "    test_labels = torch.randint(0, 2, (100,)).cuda() if torch.cuda.is_available() else torch.randint(0, 2, (100,))\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(test_data)\n",
        "\n",
        "    # Compute loss and accuracy\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == test_labels).sum().item()\n",
        "    accuracy = correct / len(test_labels)\n",
        "    test_loss = criterion(outputs, test_labels).item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woJ5GNm1CHZX",
        "outputId": "75fd1680-28da-469d-d0d6-08bfe47b6682"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7549, Accuracy: 45.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')\n",
        "print(\"Model saved to 'model.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWky0GctCbfG",
        "outputId": "fafa5a03-0a9b-4eb0-c822-98ab1dd09368"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install websockets pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDW4w_DeDGfP",
        "outputId": "ca09daef-786a-4363-b9c3-047f5a8f4fa5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting websockets\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, websockets\n",
            "Successfully installed pydub-0.25.1 websockets-13.1\n"
          ]
        }
      ]
    }
  ]
}